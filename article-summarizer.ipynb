{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Final Project: Article Summarizer\n",
    "\n",
    "### Student Name: Arnold Atchoe.\n",
    "### GitHub Repository: https://github.com/kwameape123/module7-final_project\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "You should bring in code from previous assignments to help you answer the questions below.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Description\n",
    "This project involves identifying common themes or entities across the news websites listed below and then analysing an article on the most common entities.\n",
    "1.\thttps://www.cnn.com/world\n",
    "2.\thttps://www.bbc.com/news\n",
    "3.\thttps://www.foxnews.com/world\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Libraries\n",
    "Libraries will be install in virtual environment using requirements.txt and pip.\n",
    "\n",
    "1. beautifulsoup4  # parsing webpages (HTML documents)\n",
    "2. html5lib              # parsing webpages (HTML documents)  - especially if poorly formatted\n",
    "3. ipykernel    # for Jupyter notebooks\n",
    "4. jupyterlab   # for Jupyter notebooks\n",
    "5. matplotlib           # customizing visualizations\n",
    "6. requests     # make HTTP requests (a very popular Python package)\n",
    "7. spacy                   # for NLP \n",
    "8. spacytextblob     # for NLP - combines spaCy and TextBlob (simpler interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import html5lib\n",
    "import ipykernel\n",
    "import jupyterlab\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import spacytextblob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "annotated-types           0.7.0\n",
      "anyio                     4.6.0\n",
      "argon2-cffi               23.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.argon2-cffi-bindings      21.2.0\n",
      "\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attrs                     24.2.0\n",
      "babel                     2.16.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.1.0\n",
      "blis                      1.3.0\n",
      "build                     1.2.2.post1\n",
      "catalogue                 2.0.10\n",
      "certifi                   2024.8.30\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.3.2\n",
      "click                     8.2.1\n",
      "cloudpathlib              0.21.1\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.2\n",
      "confection                0.1.5\n",
      "contourpy                 1.3.2\n",
      "cycler                    0.12.1\n",
      "cymem                     2.0.11\n",
      "debugpy                   1.8.6\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "et-xmlfile                1.1.0\n",
      "executing                 2.1.0\n",
      "fastjsonschema            2.20.0\n",
      "fonttools                 4.58.1\n",
      "fqdn                      1.5.1\n",
      "greenlet                  3.2.2\n",
      "h11                       0.14.0\n",
      "html5lib                  1.1\n",
      "httpcore                  1.0.6\n",
      "httpx                     0.27.2\n",
      "idna                      3.10\n",
      "ipykernel                 6.29.5\n",
      "ipython                   8.28.0\n",
      "ipywidgets                8.1.7\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.4\n",
      "joblib                    1.5.1\n",
      "json5                     0.9.25\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter                   1.1.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.10.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.14.2\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.4.3\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "jupyterlab_widgets        3.0.15\n",
      "kiwisolver                1.4.8\n",
      "langcodes                 3.5.0\n",
      "language_data             1.3.0\n",
      "marisa-trie               1.2.1\n",
      "markdown-it-py            3.0.0\n",
      "MarkupSafe                3.0.0\n",
      "matplotlib                3.10.3\n",
      "matplotlib-inline         0.1.7\n",
      "mdurl                     0.1.2\n",
      "mistune                   3.0.2\n",
      "murmurhash                1.0.13\n",
      "nbclient                  0.10.0\n",
      "nbconvert                 7.16.4\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "nltk                      3.9.1\n",
      "notebook                  7.4.3\n",
      "notebook_shim             0.2.4\n",
      "numpy                     2.1.1\n",
      "openpyxl                  3.1.5\n",
      "overrides                 7.7.0\n",
      "packaging                 24.1\n",
      "pandas                    2.2.2\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pillow                    11.2.1\n",
      "pip                       25.1.1\n",
      "platformdirs              4.3.6\n",
      "preshed                   3.0.10\n",
      "prometheus_client         0.21.0\n",
      "prompt_toolkit            3.0.48\n",
      "psutil                    6.0.0\n",
      "psycopg2-binary           2.9.10\n",
      "pure_eval                 0.2.3\n",
      "pycparser                 2.22\n",
      "pydantic                  2.11.7\n",
      "pydantic_core             2.33.2\n",
      "Pygments                  2.18.0\n",
      "pyparsing                 3.2.3\n",
      "pyproject_hooks           1.2.0\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        2.0.7\n",
      "pytz                      2024.2\n",
      "pywin32                   307\n",
      "pywinpty                  2.0.13\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.2.0\n",
      "referencing               0.35.1\n",
      "regex                     2025.7.34\n",
      "requests                  2.32.3\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rich                      14.1.0\n",
      "rpds-py                   0.20.0\n",
      "Send2Trash                1.8.3\n",
      "setuptools                75.1.0\n",
      "shellingham               1.5.4\n",
      "six                       1.16.0\n",
      "smart_open                7.3.0.post1\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.6\n",
      "spacy                     3.8.7\n",
      "spacy-legacy              3.0.12\n",
      "spacy-loggers             1.0.5\n",
      "spacytextblob             5.0.0\n",
      "SQLAlchemy                2.0.41\n",
      "srsly                     2.5.1\n",
      "stack-data                0.6.3\n",
      "terminado                 0.18.1\n",
      "textblob                  0.19.0\n",
      "thinc                     8.3.6\n",
      "tinycss2                  1.3.0\n",
      "tornado                   6.4.1\n",
      "tqdm                      4.67.1\n",
      "traitlets                 5.14.3\n",
      "typer                     0.16.0\n",
      "types-python-dateutil     2.9.0.20241003\n",
      "typing_extensions         4.13.2\n",
      "typing-inspection         0.4.1\n",
      "tzdata                    2024.1\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.3\n",
      "wasabi                    1.1.3\n",
      "wcwidth                   0.2.13\n",
      "weasel                    0.4.1\n",
      "webcolors                 24.8.0\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "wheel                     0.44.0\n",
      "widgetsnbextension        4.0.14\n",
      "wrapt                     1.17.2\n"
     ]
    }
   ],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FETCH CONTENTS FROM NEWS WEBSITE TO FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched content from all news websites.\n",
      "Content type of CNN: text/html; charset=utf-8\n",
      "Content type of BBC: text/html; charset=utf-8\n",
      "Content type of Fox News: text/html; charset=utf-8\n",
      "First 500 characters of CNN content:   <!DOCTYPE html>\n",
      "<html lang=\"en\" data-uri=\"cms.cnn.com/_pages/cl9wqykfb000026p95p1gfx93@published\" data-layout-uri=\"cms.cnn.com/_layouts/layout-no-rail/instances/world-v1@published\" >\n",
      "  <head>\n",
      "<link rel=\"dns-prefetch\" href=\"//tpc.googlesyndication.com\">\n",
      "\n",
      "<link rel=\"preconnect\" href=\"//tpc.googlesyndication.com\">\n",
      "\n",
      "<link rel=\"dns-prefetch\" href=\"//pagead2.googlesyndication.com\">\n",
      "\n",
      "<link rel=\"preconnect\" href=\"//pagead2.googlesyndication.com\">\n",
      "\n",
      "<link rel=\"dns-prefetch\" href=\"//www.googletagservices\n",
      "First 500 characters of BBC content: <!DOCTYPE html><html lang=\"en-GB\"><head><meta charSet=\"utf-8\"/><meta name=\"viewport\" content=\"width=device-width\"/><title>World | Latest News &amp; Updates | BBC News</title><meta name=\"page.section\" content=\"News\"/><meta name=\"page.subsection\" content=\"World\"/><meta property=\"og:title\" content=\"World | Latest News &amp; Updates | BBC News\"/><meta name=\"twitter:title\" content=\"World | Latest News &amp; Updates | BBC News\"/><meta name=\"description\" content=\"Get all the latest news, live updates a\n",
      "First 500 characters of Fox News content: <!doctype html>\n",
      "<html data-n-head-ssr lang=\"en\" data-n-head=\"%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D\">\n",
      "  <head>\n",
      "    <title>World News Today Updates &amp; Daily Headlines  | Fox News</title><meta data-n-head=\"ssr\" http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\"><meta data-n-head=\"ssr\" data-hid=\"robots\" name=\"robots\" content=\"index, follow, max-image-preview:large, max-video-preview:-1\"><meta data-n-head=\"ssr\" data-hid=\"content-type\" http-equiv=\"content-type\" content=\"text/html; charset=\n"
     ]
    }
   ],
   "source": [
    "# fetch HTML content from multiple news websites\n",
    "response_1 = requests.get(\"https://www.cnn.com/world\")\n",
    "response_2 = requests.get(\"https://www.bbc.com/news/world\")\n",
    "response_3 = requests.get(\"https://www.foxnews.com/world\")\n",
    "\n",
    "# check if the requests were successful\n",
    "if response_1.status_code == 200 and response_2.status_code == 200 and response_3.status_code == 200:\n",
    "    print(\"Successfully fetched content from all news websites.\")\n",
    "else:print(\"Failed to fetch content from one or more news websites.\")\n",
    "\n",
    "# check the content type\n",
    "print(\"Content type of CNN:\", response_1.headers['Content-Type'])\n",
    "print(\"Content type of BBC:\", response_2.headers['Content-Type'])\n",
    "print(\"Content type of Fox News:\", response_3.headers['Content-Type'])\n",
    "\n",
    "# view the first 500 characters of the HTML content\n",
    "print(\"First 500 characters of CNN content:\", response_1.text[:500])\n",
    "print(\"First 500 characters of BBC content:\", response_2.text[:500])\n",
    "print(\"First 500 characters of Fox News content:\", response_3.text[:500])\n",
    "\n",
    "# save the HTML content to pickle files to avoid re-fetching\n",
    "import pickle\n",
    "with open('cnn_content.pkl', 'wb') as file:\n",
    "    pickle.dump(response_1.text, file)\n",
    "with open('bbc_content.pkl', 'wb') as file:\n",
    "    pickle.dump(response_2.text, file)\n",
    "with open('foxnews_content.pkl', 'wb') as file:\n",
    "    pickle.dump(response_3.text, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ AND PARSE CONTENT OF PICKLE FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read contents from the pickle files\n",
    "with open('cnn_content.pkl', 'rb') as file:\n",
    "    cnn_content = pickle.load(file)\n",
    "with open('bbc_content.pkl', 'rb') as file:\n",
    "    bbc_content = pickle.load(file)\n",
    "with open('foxnews_content.pkl', 'rb') as file:\n",
    "    foxnews_content = pickle.load(file)\n",
    "\n",
    "# parse the HTML content using BeautifulSoup\n",
    "cnn_soup = BeautifulSoup(cnn_content, 'html5lib')\n",
    "bbc_soup = BeautifulSoup(bbc_content, 'html5lib')\n",
    "foxnews_soup = BeautifulSoup(foxnews_content, 'html5lib')\n",
    "\n",
    "# find headlines \n",
    "cnn_headlines = cnn_soup.find_all(class_='container__headline-text')\n",
    "bbc_headlines = bbc_soup.find_all(attrs={'data-testid':'card-headline'})\n",
    "foxnews_headlines = foxnews_soup.find_all(class_='title')\n",
    "# write the headlines to a text file\n",
    "with open('cnn_headlines.txt', 'w') as file:\n",
    "    for headline in cnn_headlines:\n",
    "        file.write(headline.get_text() + '\\n')\n",
    "with open('bbc_headlines.txt', 'w') as file:\n",
    "    for headline in bbc_headlines:\n",
    "        file.write(headline.get_text() + '\\n')\n",
    "with open('foxnews_headlines.txt', 'w') as file:\n",
    "    for headline in foxnews_headlines:\n",
    "        file.write(headline.get_text() + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DETERMINE THE MOST COMMON HEADLINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top ten headline_counts:\n",
      "Russia drops missile deployment moratorium amid tensions with Trump admin: 3 occurrences\n",
      "UN official says Israel expanding Gaza operations would risk 'catastrophic consequences': 2 occurrences\n",
      "Despite Trump's peace calls, Russian attacks on Ukraine double since inauguration: 2 occurrences\n",
      "Texas governor seeks court order to fire top Democrat who fled in row over voting map: 2 occurrences\n",
      "RFK Jr cancels $500m in funding for mRNA vaccines that counter viruses like Covid: 2 occurrences\n",
      "Hiroshima marks 80 years since atomic bombing: 2 occurrences\n",
      "Great Barrier Reef suffers worst coral decline on record: 2 occurrences\n",
      "Clintons subpoenaed to testify in congressional Epstein investigation: 2 occurrences\n",
      "Brother of Israeli hostage urges UN to act after video shows Hamas starving and torturing captives: 2 occurrences\n",
      "Johnson dines with Netanyahu in landmark visit, highest US official to visit disputed West Bank: 2 occurrences\n",
      "\n",
      "Most common headline: Russia drops missile deployment moratorium amid tensions with Trump admin\n"
     ]
    }
   ],
   "source": [
    "# load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# extract common headlines among the three news websites from text files\n",
    "with open('cnn_headlines.txt', 'r') as file:\n",
    "    cnn_headlines = file.readlines()\n",
    "with open('bbc_headlines.txt', 'r') as file:\n",
    "    bbc_headlines = file.readlines()\n",
    "with open('foxnews_headlines.txt', 'r') as file:\n",
    "    foxnews_headlines = file.readlines()\n",
    "# combine all headlines into a single list\n",
    "all_headlines = cnn_headlines + bbc_headlines + foxnews_headlines\n",
    "# create a dictionary to count occurrences of each headline\n",
    "headline_counts = {}\n",
    "for headline in all_headlines:\n",
    "    headline = headline.strip()\n",
    "    if headline in headline_counts:\n",
    "        headline_counts[headline] += 1\n",
    "    else:\n",
    "        headline_counts[headline] = 1\n",
    "    \n",
    "# print top ten headlines and find the most common headline\n",
    "most_common_headline = max(headline_counts, key=headline_counts.get)\n",
    "print(\"top ten headline_counts:\")\n",
    "top_ten_headlines = sorted(headline_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for headline, count in top_ten_headlines:\n",
    "    print(f\"{headline}: {count} occurrences\")\n",
    "print()\n",
    "print(\"Most common headline:\", most_common_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find on the internet an article or blog post about a topic that interests you and you are able to get the text for using the technologies we have applied in the course.  Get the html for the article and store it in a file (which you must submit with your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read in your article's html source from the file you created in question 1 and do sentiment analysis on the article/post's text (use `.get_text()`).  Print the polarity score with an appropriate label.  Additionally print the number of sentences in the original article (with an appropriate label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using the histograms from questions 5 and 6, decide a \"cutoff\" score for tokens and lemmas such that fewer than half the sentences would have a score greater than the cutoff score.  Record the scores in this Markdown cell\n",
    "\n",
    "* Cutoff Score (tokens): \n",
    "* Cutoff Score (lemmas):\n",
    "\n",
    "Feel free to change these scores as you generate your summaries.  Ideally, we're shooting for at least 6 sentences for our summary, but don't want more than 10 (these numbers are rough estimates; they depend on the length of your article)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on tokens) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Print the polarity score of your summary you generated with the token scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on lemmas) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Print the polarity score of your summary you generated with the lemma scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.  Compare your polarity scores of your summaries to the polarity scores of the initial article.  Is there a difference?  Why do you think that may or may not be?.  Answer in this Markdown cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Based on your reading of the original article, which summary do you think is better (if there's a difference).  Why do you think this might be?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
